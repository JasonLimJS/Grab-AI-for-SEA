{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grab AI for SEA Challenge- kNN Model 2 (Training on 1- Day Lagged Data with PCA).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLimJS/Grab-AI-for-SEA/blob/master/Model%202%20of%20Ensemble%20kNN%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M3hwAuqvm7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy import stats\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "drive.mount('drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzYWcoCqwkE9",
        "colab_type": "text"
      },
      "source": [
        "Create 1- day lagged demand:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPqN4Xj3woIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw= pd.read_csv('drive/My Drive/raw.csv',index_col=0)\n",
        "\n",
        "raw.sort_values(by=['geohash6','time_min','day'],inplace=True)\n",
        "\n",
        "\n",
        "raw['demand_lag_id']= raw.groupby(['geohash6','time_min']).demand.shift(1)\n",
        "\n",
        "col_arrangement= ['geohash6', 'day', 'time_min', 'timestamp', 'lat', 'long', 'hour',\n",
        "                  'minute','demand', 'demand_lag_id']\n",
        "\n",
        "\n",
        "raw= raw[col_arrangement]\n",
        "\n",
        "#For those data points which timestamp only has 1 day being observed, its demand is taken as lagged demand  \n",
        "\n",
        "obs_count= pd.DataFrame(raw.groupby(['geohash6','time_min']).demand.count())\n",
        "obs_count.reset_index(level=[0,1],drop=False,inplace=True)\n",
        "obs_count.columns=['geohash6','time_min','demand_count']\n",
        "\n",
        "raw_combin= pd.merge(left=raw,right=obs_count,left_on=['geohash6','time_min'],right_on=['geohash6','time_min'])\n",
        "raw_combin['demand_lag_1']= np.where(raw_combin.demand_count==1,raw_combin.demand,raw_combin.demand_lag_id)\n",
        "\n",
        "\n",
        "\n",
        "arr_sample= ['geohash6', 'day', 'time_min', 'demand_count', 'lat', 'long', 'hour',\n",
        "             'minute', 'demand_lag_id', 'demand', 'demand_lag_1']\n",
        "\n",
        "raw_combin=raw_combin[arr_sample]\n",
        "\n",
        "all_lag_1d= raw_combin[['geohash6','lat','long','time_min','day','demand_lag_1','demand']]\n",
        "all_lag_1d.sort_values(by=['geohash6','time_min','day'],inplace=True)\n",
        "all_lag_1d.dropna(inplace=True)\n",
        "\n",
        "all_lag_1d.to_csv('all_lag_1d.csv')\n",
        "!cp all_lag_1d.csv drive/My\\ Drive/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTW-LgvWxGbK",
        "colab_type": "text"
      },
      "source": [
        "Model 1: kNN without lag, without PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QydbikOzxIpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_lag_1d= pd.read_csv('drive/My Drive/all_lag_1d.csv',index_col=0)\n",
        "\n",
        "arranged_col= ['lat','long','time_min','demand_lag_1']\n",
        "X= all_lag_1d[arranged_col].values\n",
        "y= all_lag_1d.demand.values\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, random_state= 7, \\\n",
        "                                                   test_size=0.20,stratify=X[:,2])\n",
        "\n",
        "\n",
        "steps=[('scaler',StandardScaler()),('pca',PCA(n_components=3))]\n",
        "\n",
        "norm_pca= Pipeline(steps)\n",
        "\n",
        "X_train_scaled= norm_pca.fit_transform(X_train)\n",
        "\n",
        "norm_mean= norm_pca.named_steps['scaler'].mean_\n",
        "norm_sd= norm_pca.named_steps['scaler'].var_**0.5\n",
        "print(norm_mean)\n",
        "print(norm_sd)\n",
        "\n",
        "pca_mean= norm_pca.named_steps['pca'].mean_\n",
        "pca_comp= norm_pca.named_steps['pca'].components_\n",
        "print(pca_mean)\n",
        "print(pca_comp)\n",
        "\n",
        "X_test_scaled= norm_pca.transform(X_test)\n",
        "\n",
        "Model1= \"Model 2\"\n",
        "\n",
        "train_test_kNN(X_train_scaled,X_test_scaled,y_train,y_test, Model1, 60)\n",
        "\n",
        "\n",
        "######################## Best Model after Hyperparameter Tuning is : #######################\n",
        "\n",
        "#Training time: 25882.505648851395 secs, R2 value: 0.8242358197645516, RSME: 0.06711758631654619\n",
        "\n",
        "#KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "#                    metric_params=None, n_jobs=None, n_neighbors=45, p=2,\n",
        "#                    weights='distance')\n",
        "\n",
        "# norm_mean= np.array([-5.34740478e+00,9.07638854e+01,6.10487686e+02,1.05574095e-01])\n",
        "#  norm_std= np.array([5.65621040e-02,1.02704766e-01,3.92263089e+02,1.59564436e-01])\n",
        "#  pca_mean= np.array([ 6.14163081e-15 ,1.66324146e-14 ,2.60231469e-16,-2.49625107e-14])\n",
        "#  pca_comp= np.array([[ 0.64256504 ,0.72425171, -0.21969293, -0.11960199],\n",
        "#                    [-0.21087891,  0.07957761, -0.71483013,  0.66197838],\n",
        "#                    [ 0.51445685, -0.2009283,   0.46599514,  0.69123838]])\n",
        "\n",
        "\n",
        "\n",
        "whole_lag_1d_kNN= KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "                    metric_params=None, n_jobs=None, n_neighbors=45, p=2,\n",
        "                   weights='distance')\n",
        "\n",
        "whole_lag_1d_kNN.fit(X_train_scaled,y_train)\n",
        "\n",
        "joblib.dump(whole_lag_1d_kNN,'whole_lag_1d_kNN.pkl')\n",
        "!cp whole_lag_1d_kNN.pkl drive/My\\ Drive/\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}